{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TopicModelingWithSVDandNMF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM0o22KMhUpRN2qWiROv8+/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kunal077/Natural-Language-Processing/blob/main/Natural%20Language%20Processing%20Series%20/%20TopicModelingWithSVDandNMF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzCBgPB5jisT"
      },
      "source": [
        "In topic modelling the goal is to find the TOPIC which occurs in a piece of text, for example in a paragraph, we can have multiple words or phrases that can be clubbed together under one TOPIC, so that is TOPIC Modelling, undetstanding what topic is what."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wS0s2-buj3PF"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn import decomposition\n",
        "from scipy import linalg\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt50qMggfn8G"
      },
      "source": [
        "%matplotlib inline\n",
        "np.set_printoptions(suppress = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryoPhsYffvKk",
        "outputId": "bb4f3b88-cd2b-4675-b746-c61cce143d3c"
      },
      "source": [
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "print(list(newsgroups_train.target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVcKPl-agpMX"
      },
      "source": [
        "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
        "remove = ('headers', 'footers', 'quotes')\n",
        "#We are removing these words so that they do not bother us in overfitting the classifier\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=remove)\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=remove)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSZX-mvehWwg",
        "outputId": "88a28628-49ba-4e9d-ef6b-f41d5fc6f9d8"
      },
      "source": [
        "newsgroups_train.filenames.shape, newsgroups_train.target.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2034,), (2034,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yFYhnd7hdnV",
        "outputId": "08370728-2951-4078-f7b2-d7d8e236f785"
      },
      "source": [
        "print(newsgroups_train.filenames); print()\n",
        "print(newsgroups_train.target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/root/scikit_learn_data/20news_home/20news-bydate-train/comp.graphics/38816'\n",
            " '/root/scikit_learn_data/20news_home/20news-bydate-train/talk.religion.misc/83741'\n",
            " '/root/scikit_learn_data/20news_home/20news-bydate-train/sci.space/61092'\n",
            " ...\n",
            " '/root/scikit_learn_data/20news_home/20news-bydate-train/comp.graphics/38737'\n",
            " '/root/scikit_learn_data/20news_home/20news-bydate-train/alt.atheism/53237'\n",
            " '/root/scikit_learn_data/20news_home/20news-bydate-train/comp.graphics/38269']\n",
            "\n",
            "[1 3 2 ... 1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkskkRiIipNR",
        "outputId": "e8f615dd-5307-49e8-a811-832460749816"
      },
      "source": [
        "print(np.array(newsgroups_train.target_names)[newsgroups_train.target[:10]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['comp.graphics' 'talk.religion.misc' 'sci.space' 'alt.atheism'\n",
            " 'sci.space' 'alt.atheism' 'sci.space' 'comp.graphics' 'sci.space'\n",
            " 'comp.graphics']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MHRR1pRjELM",
        "outputId": "081f9893-fedd-4d0f-e218-70595eeaa82d"
      },
      "source": [
        "print(newsgroups_train.target[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 3 2 0 2 0 2 1 2 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzoA3ASsj7Sr"
      },
      "source": [
        "#now we set some custom numbers of topic that we want\n",
        "#There is no truth here, since this is purely a case of \n",
        "#Unsupervised Learning, so we set how many topics i want.\n",
        "num_topics, num_top_words = 6, 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-2aTGW5kgS9"
      },
      "source": [
        "\n",
        "Stop Words\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Some extremely common words which would appear to be of little value in helping select documents matching a user need, they are excluded from the vocabulary entirely, These words are called Stop Words.\n",
        "\n",
        "---\n",
        "\n",
        "The general trend in IR Systems over time has been from the standard use of quire large stop words lists (200-300) terms in there) to very small stop lists (7-12), now Web Search Engines do not use Stop Lists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zifZlMvlQGI",
        "outputId": "872285f6-35a5-41dd-a9d8-b275c7c20980"
      },
      "source": [
        "from sklearn.feature_extraction import stop_words\n",
        "print(len(sorted(list(stop_words.ENGLISH_STOP_WORDS))))\n",
        "print(sorted(list(stop_words.ENGLISH_STOP_WORDS))[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "318\n",
            "['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de', 'describe', 'detail', 'do', 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IakvfFQdlzVB"
      },
      "source": [
        "Stemming and Lemmatization\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "They both generate the ROOT form of the words.\n",
        "Lemmatization uses the rules about a language and resulting tokens are all actual words.\n",
        "\n",
        "Stemming is poor Lemmatization, crude Heuristic that chops the ends of  of words and the resulting tokens may not be actual words, Stemming is faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8FXjTLQkcw9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eb2c0b8-1b92-41ee-ee91-b7843173ca79"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "#wordnet is an English Dictionary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHv9OqhvQ2Mq"
      },
      "source": [
        "from nltk import stem\n",
        "#We are using nltk becuase it has both Lem and Stem\n",
        "#Spacy only has Lem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v53P0c6jQ6EZ"
      },
      "source": [
        "wnl = stem.WordNetLemmatizer()\n",
        "porter = stem.porter.PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0i_HWX_XTmZ1",
        "outputId": "000fda62-d4e1-4a42-9ea9-20a8d411c627"
      },
      "source": [
        "word_list = ['feet', 'foot', 'foots', 'footing']\n",
        "[wnl.lemmatize(word) for word in word_list]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['foot', 'foot', 'foot', 'footing']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfbXibX5T2Sg",
        "outputId": "18a328e3-5450-487c-ad53-7aba17c36e60"
      },
      "source": [
        "[porter.stem(word) for word in word_list]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['feet', 'foot', 'foot', 'foot']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwPxdJhiT9-X",
        "outputId": "ebffead8-abe7-42d8-f551-a106ceb75f78"
      },
      "source": [
        "word_list = ['organize', 'organizes', 'organizing']\n",
        "word_list2 = ['universe', 'university']\n",
        "print([wnl.lemmatize(word) for word in word_list])\n",
        "print([porter.stem(word) for word in word_list])\n",
        "print([wnl.lemmatize(word) for word in word_list2])\n",
        "print([porter.stem(word) for word in word_list2])\n",
        "#Lemmatizing makes more sense as compared to Stemming\n",
        "#More morphological languages are better for Lemmatization and Stemming"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['organize', 'organizes', 'organizing']\n",
            "['organ', 'organ', 'organ']\n",
            "['universe', 'university']\n",
            "['univers', 'univers']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9rM6QFwVPF3",
        "outputId": "be87be0b-f60b-4d8d-b0c3-12cfba1f7573"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "inSpacy = nlp.Defaults.stop_words - stop_words.ENGLISH_STOP_WORDS\n",
        "inSklearn = stop_words.ENGLISH_STOP_WORDS - nlp.Defaults.stop_words\n",
        "print(inSpacy, \"\\n\", inSklearn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'‘re', '‘ll', '‘s', \"'s\", \"'m\", \"'d\", '’ll', 'n’t', 'regarding', 'did', 'doing', \"'ll\", 'just', '‘d', '’s', 'does', 'make', \"'ve\", '’d', 'unless', 'used', '’m', \"n't\", '’ve', '‘m', 'using', '’re', 'n‘t', 'say', 'really', 'ca', \"'re\", '‘ve', 'various', 'quite'} \n",
            " frozenset({'fill', 'de', 'un', 'sincere', 'etc', 'hasnt', 'system', 'found', 'con', 'cry', 'co', 'describe', 'bill', 'interest', 'mill', 'eg', 'couldnt', 'thick', 'inc', 'fire', 'cant', 'amoungst', 'ie', 'detail', 'thin', 'ltd', 'find'})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnCfS0hwY0QI"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEjfG07YZV0Z"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "class LemmaTokenizer(object):\n",
        "  def __init__ (self):\n",
        "    self.wnl = stem.WordNetLemmatizer()\n",
        "  \n",
        "  def __call__ (self, doc):\n",
        "    return [self.wnl.Lemmatize(t) for t in word_tokenize(doc)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2epVTvUtbovh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}